import os
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler


def shuffle(array: np.ndarray):
    from numpy.random import MT19937
    from numpy.random import RandomState, SeedSequence
    np_rs = RandomState(MT19937(SeedSequence(123456789)))
    np_rs.shuffle(array)


def read_dataframe(filename, sep=",", engine=None):
    if type(filename) == list:
        print(filename)
        df_list = [
            pd.read_csv(f, sep=sep, header=None, names=None, engine=engine)
                for f in filename
        ]
        df = pd.concat(df_list, ignore_index=True)
        filename = filename[0]
    else:
        df = pd.read_csv(filename, sep=sep, 
                    header=None, names=None, engine=engine)
    return df
   

def read_geant4(filename, sep=" ", engine=None):
    filename = "/global/homes/y/yanglyu/phys_290/MCGenerators/G4/HadronicInteractions/build/" + filename
    df = pd.read_csv(filename, sep=sep, usecols=(1,2,3,4,5)) # 4 vector and num_secondary

    print(df.head())

    data = df.to_numpy().astype(np.float32)
    print(data.shape)
    training_size = int(data.shape[0]*0.8)

    X_train, X_test = data[:training_size, :-1], data[training_size:, :-1]
    y_train, y_test = data[:training_size, -1], data[training_size:, -1]

    y_train = y_train[..., None]
    y_test = y_test[..., None]

    xlabels = ['num_secondary']
    print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

    return (X_train, X_test, y_train, y_test, xlabels)

    

def herwig_angles(filename,
        max_evts=None, testing_frac=0.1):
    """
    This reads the Herwig dataset where one cluster decays
    into two particles.
    In this case, we ask the GAN to predict the theta and phi
    angle of one of the particles
    """
    df = read_dataframe(filename, engine='python')

    event = None
    with open(filename, 'r') as f:
        for line in f:
            event = line
            break
    particles = event[:-2].split(';')

    input_4vec = df[0].str.split(",", expand=True)[[4, 5, 6, 7]].to_numpy().astype(np.float32)
    out_particles = []
    for idx in range(1, len(particles)):
        out_4vec = df[idx].str.split(",", expand=True).to_numpy()[:, -4:].astype(np.float32)
        out_particles.append(out_4vec)

    # ======================================
    # Calculate the theta and phi angle 
    # of the first outgoing particle
    # ======================================
    out_4vec = out_particles[0]
    px = out_4vec[:, 1].astype(np.float32)
    py = out_4vec[:, 2].astype(np.float32)
    pz = out_4vec[:, 3].astype(np.float32)
    pT = np.sqrt(px**2 + py**2)
    phi = np.arctan(px/py)
    theta = np.arctan(pT/pz)

    # <NOTE, inputs and outputs are scaled to be [-1, 1]>
    max_phi = np.max(np.abs(phi))
    max_theta = np.max(np.abs(theta))
    scales = np.array([max_phi, max_theta], np.float32)

    truth_in = np.stack([phi, theta], axis=1) / scales

    shuffle(truth_in)
    shuffle(input_4vec)


    # Split the data into training and testing
    # <HACK, FIXME, NOTE>
    # <HACK, For now a maximum of 10,000 events are used for testing, xju>
    num_test_evts = int(input_4vec.shape[0]*testing_frac)
    if num_test_evts < 10_000: num_test_evts = 10_000

    # <NOTE, https://numpy.org/doc/stable/reference/random/generated/numpy.random.seed.html>



    test_in, train_in = input_4vec[:num_test_evts], input_4vec[num_test_evts:max_evts]
    test_truth, train_truth = truth_in[:num_test_evts], truth_in[num_test_evts:max_evts]

    xlabels = ['phi', 'theta']

    return (train_in, train_truth, test_in, test_truth, xlabels)

def herwig_angles2(filename,
        max_evts=None, testing_frac=0.1, mode=2):
    """
    This Herwig dataset is for the "ClusterDecayer" study.
    Each event has q1, q1, cluster, h1, h2.
    I define 3 modes:
    0) both q1, q2 are with Pert=1
    1) only one of q1 and q2 is with Pert=1
    2) neither q1 nor q2 are with Pert=1
    3) at least one quark with Pert=1
    """
    if type(filename) == list:
        filename = filename[0]
    arrays = np.load(filename)
    truth_in = arrays['out_truth']
    input_4vec = arrays['input_4vec']

    shuffle(truth_in)
    shuffle(input_4vec)
    print(truth_in.shape, input_4vec.shape)


    # Split the data into training and testing
    # <HACK, FIXME, NOTE>
    # <HACK, For now a maximum of 10,000 events are used for testing, xju>
    num_test_evts = int(input_4vec.shape[0]*testing_frac)
    if num_test_evts < 10_000: num_test_evts = 10_000

    test_in, train_in = input_4vec[:num_test_evts], input_4vec[num_test_evts:max_evts]
    test_truth, train_truth = truth_in[:num_test_evts], truth_in[num_test_evts:max_evts]
    xlabels = ['phi', 'theta']

    return (train_in, train_truth, test_in, test_truth, xlabels)


def dimuon_inclusive(filename, max_evts=None, testing_frac=0.1):
    
    df = read_dataframe(filename, " ", None)
    truth_data = df.to_numpy().astype(np.float32)
    print(f"reading dimuon {df.shape[0]} events from file {filename}")

    scaler = MinMaxScaler(feature_range=(-1,1))
    truth_data = scaler.fit_transform(truth_data)
    # scales = np.array([10, 1, 1, 10, 1, 1], np.float32)
    # truth_data = truth_data / scales

    shuffle(truth_data)

    num_test_evts = int(truth_data.shape[0]*testing_frac)
    if num_test_evts > 10_000: num_test_evts = 10_000


    test_truth, train_truth = truth_data[:num_test_evts], truth_data[num_test_evts:max_evts]

    xlabels = ['leading Muon {}'.format(name) for name in ['pT', 'eta', 'phi']] +\
              ['subleading Muon {}'.format(name) for name in ['pT', 'eta', 'phi']]
    
    return (None, train_truth, None, test_truth, xlabels)


if __name__== '__main__':
    read_geant4('pion_25GeV_Fe_100K.csv')